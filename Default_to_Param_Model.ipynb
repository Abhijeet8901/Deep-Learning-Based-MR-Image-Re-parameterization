{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Default-to-Param Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "18eREXg5LD9BOKYrfVXaidP6UiI_Z1Eoq",
      "authorship_tag": "ABX9TyMnCTbWcV8iuLTYTLyL022t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1jNG7nrWBRI"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDi4B_vKBgYx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import sampler\n",
        "import torchvision.datasets as dset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pickle\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import random\n",
        " \n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab\n",
        "from skimage.io import imsave\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torchsummary import summary\n",
        " \n",
        "from itertools import product\n",
        "from math import log10, sqrt\n",
        " \n",
        "dtype = torch.cuda.FloatTensor\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available()==False:\n",
        "  dtype=torch.FloatTensor\n",
        "print(device,dtype) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sUKygznWEIn"
      },
      "source": [
        "#Set Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH3HlAB6CG-y"
      },
      "source": [
        "class set_config:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.cuda=torch.cuda.is_available()\n",
        "    self.weight_decay=0 \n",
        "    self.lr=1e-3 # learning rate\n",
        "    self.test_img_name=None \n",
        "    self.batch_size=8 \n",
        "    self.mode='train'  # 3 modes, 'train', 'test', 'detect'\n",
        "    self.resume=False # if resume is set to true, resumes training from pretrained weights.\n",
        "    self.dir='drive/MyDrive/Projects/K-Parameter/' # path to root directory\n",
        "    self.param_weight_path=glob.glob(self.dir+'weights/latest_KParameter*')  # path to param-net model weights\n",
        "    assert len(self.param_weight_path)<=1, \"Multiple Param weight files detected.\"\n",
        "    if len(self.param_weight_path)==1:\n",
        "      self.resume=True\n",
        "      self.param_weight_path=self.param_weight_path[0]\n",
        "    else:\n",
        "      self.param_weight_path=None\n",
        "    self.end_weight_path=glob.glob(self.dir+'weights/latest_END*') # path to end(autoencoder) model weights\n",
        "    assert len(self.end_weight_path)<=1, \"Multiple END weight files detected.\"\n",
        "    if len(self.end_weight_path)==1:\n",
        "      self.end_weight_path=self.end_weight_path[0]\n",
        "    else:\n",
        "      self.end_weight_path=None\n",
        "    self.te_def=0.05 # default value for TE(echo time)\n",
        "    self.tr_def=4.5 # default value for TR(repitition time)\n",
        "    self.train_file_name=\"Train_1000\" # folder name in 'dataset/' folder to training images\n",
        "    self.test_file_name=\"brainweb images\" # folder name in 'dataset/' folder to test images\n",
        "\n",
        "config=set_config()\n",
        "\n",
        "if config.param_weight_path is not None:\n",
        "  print(\"Param Weight file detected.\",config.param_weight_path)\n",
        "else:\n",
        "  print(\"No Param weight file detected.\")\n",
        "if config.end_weight_path is not None:\n",
        "  print(\"END Weight file detected.\",config.end_weight_path)\n",
        "else:\n",
        "  print(\"No END weight file detected.\")\n",
        "  #assert False, \"No END weight file detected.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiL4I85ntBie"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjn6RU4YlQg-"
      },
      "source": [
        "class END(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.conv1=nn.Conv2d(1,64,3,stride=1,padding=1,bias=False)\n",
        "    self.relu1=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv2=nn.Conv2d(64,64,3,stride=2,padding=1,bias=False)\n",
        "    self.bn2=nn.BatchNorm2d(64,momentum=0.5)\n",
        "    self.relu2=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv3=nn.Conv2d(64,128,3,stride=2,padding=1,bias=False)\n",
        "    self.bn3=nn.BatchNorm2d(128,momentum=0.5)\n",
        "    self.relu3=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv4=nn.Conv2d(128,256,3,stride=2,padding=1,bias=False)\n",
        "    self.bn4=nn.BatchNorm2d(256,momentum=0.5)\n",
        "    self.relu4=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv5=nn.Conv2d(256,512,3,stride=2,padding=1,bias=False)\n",
        "    self.bn5=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu5=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv6=nn.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
        "    self.bn6=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu6=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv7=nn.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
        "    self.bn7=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu7=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv8=nn.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
        "    self.bn8=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu8=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv9=nn.ConvTranspose2d(512,512,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn9=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu9=nn.ReLU()\n",
        "      \n",
        "    self.conv10=nn.ConvTranspose2d(512,512,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn10=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu10=nn.ReLU()\n",
        "\n",
        "    self.conv11=nn.ConvTranspose2d(512,512,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn11=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu11=nn.ReLU()\n",
        "\n",
        "    self.conv12=nn.ConvTranspose2d(512,256,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn12=nn.BatchNorm2d(256,momentum=0.5)\n",
        "    self.relu12=nn.ReLU()\n",
        "\n",
        "    self.conv13=nn.ConvTranspose2d(256,128,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn13=nn.BatchNorm2d(128,momentum=0.5)\n",
        "    self.relu13=nn.ReLU()\n",
        "\n",
        "    self.conv14=nn.ConvTranspose2d(128,64,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn14=nn.BatchNorm2d(64,momentum=0.5)\n",
        "    self.relu14=nn.ReLU()\n",
        "\n",
        "    self.conv15=nn.ConvTranspose2d(64,32,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn15=nn.BatchNorm2d(32,momentum=0.5)\n",
        "    self.relu15=nn.ReLU()\n",
        "      \n",
        "    self.conv16=nn.Conv2d(32,16,3,stride=1,padding=1,bias=False)\n",
        "    self.bn16=nn.BatchNorm2d(16,momentum=0.5)\n",
        "    self.relu16=nn.ReLU()\n",
        "\n",
        "    self.conv17=nn.Conv2d(16,1,1,stride=1,bias=False)\n",
        "\n",
        "  def forward(self,img):\n",
        "\n",
        "    if self.training:\n",
        "\n",
        "      x1=self.relu1(self.conv1(img))\n",
        "\n",
        "      x2=self.relu2(self.bn2(self.conv2(x1)))\n",
        "\n",
        "      x3=self.relu3(self.bn3(self.conv3(x2)))\n",
        "\n",
        "      x4=self.relu4(self.bn4(self.conv4(x3)))\n",
        "\n",
        "      x5=self.relu5(self.bn5(self.conv5(x4)))\n",
        "\n",
        "      x6=self.relu6(self.bn6(self.conv6(x5)))\n",
        "\n",
        "      x7=self.relu7(self.bn7(self.conv7(x6)))\n",
        "\n",
        "      x8=self.relu8(self.bn8(self.conv8(x7)))\n",
        "\n",
        "      x9=self.relu9(self.bn9(self.conv9(x8)))\n",
        "\n",
        "      x10=self.relu10(self.bn10(self.conv10(x9)))\n",
        "\n",
        "      x11=self.relu11(self.bn11(self.conv11(x10)))\n",
        "\n",
        "      x12=self.relu12(self.bn12(self.conv12(x11)))\n",
        "\n",
        "      x13=self.relu13(self.bn13(self.conv13(x12)))\n",
        "\n",
        "      x14=self.relu14(self.bn14(self.conv14(x13)))\n",
        "\n",
        "      x15=self.relu15(self.bn15(self.conv15(x14)))\n",
        "\n",
        "      x16=self.relu16(self.bn16(self.conv16(x15)))\n",
        "        \n",
        "      x17=self.conv17(x16)\n",
        "      x17=torch.tanh(x17)\n",
        "      \n",
        "      return x17\n",
        "    \n",
        "    else:\n",
        "\n",
        "      outs=[]\n",
        "\n",
        "      x1=self.relu1(self.conv1(img))\n",
        "      outs.append(x1)\n",
        "\n",
        "      x2=self.relu2(self.bn2(self.conv2(x1)))\n",
        "      outs.append(x2)\n",
        "\n",
        "      x3=self.relu3(self.bn3(self.conv3(x2)))\n",
        "      outs.append(x3)\n",
        "\n",
        "      x4=self.relu4(self.bn4(self.conv4(x3)))\n",
        "      outs.append(x4)\n",
        "\n",
        "      x5=self.relu5(self.bn5(self.conv5(x4)))\n",
        "      outs.append(x5)\n",
        "\n",
        "      x6=self.relu6(self.bn6(self.conv6(x5)))\n",
        "      outs.append(x6)\n",
        "\n",
        "      x7=self.relu7(self.bn7(self.conv7(x6)))\n",
        "      outs.append(x7)\n",
        "\n",
        "      x8=self.relu8(self.bn8(self.conv8(x7)))\n",
        "      outs.append(x8)\n",
        "\n",
        "      return outs\n",
        "  \n",
        "  def init_weights(self):\n",
        "    for name,module in self.named_modules():\n",
        "      if isinstance(module,nn.Conv2d) or isinstance(module,nn.ConvTranspose2d):\n",
        "        nn.init.xavier_uniform_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "          module.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERp0qnvozAJK"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,in_channels,out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels=in_channels  # no of channels of input\n",
        "    self.out_channels=out_channels # required no of channels of output\n",
        "    # as each block of param-net has different input and output size, therefore we had to generalize the no of input and output channels\n",
        "\n",
        "    self.conv_t=nn.ConvTranspose2d(in_channels,in_channels,3,stride=2,padding=1,output_padding=1,bias=False)  # 2x*2x*in_channels\n",
        "    self.conv1=nn.Conv2d(2*in_channels,in_channels,3,stride=1,padding=1,bias=False)                           # 2x*2x*in_channels\n",
        "    self.in1=nn.InstanceNorm2d(in_channels)\n",
        "    self.relu1=nn.LeakyReLU(0.2)\n",
        "    self.conv2=nn.Conv2d(2*in_channels,in_channels,3,stride=1,padding=1,bias=False)                           # 2x*2x*in_channels\n",
        "    self.in2=nn.InstanceNorm2d(in_channels)\n",
        "    self.relu2=nn.LeakyReLU(0.2)\n",
        "    self.conv3=nn.Conv2d(in_channels,out_channels,3,stride=1,padding=1,bias=False)                           # 2x*2x*out_channels\n",
        "    self.in3=nn.InstanceNorm2d(out_channels)\n",
        "    self.relu3=nn.LeakyReLU(0.2)\n",
        "\n",
        "  def forward(self,input,brain_features,params):\n",
        "\n",
        "    # brain features: image features from END model(autoencoder model)\n",
        "    # params : parameters of required output\n",
        "    \n",
        "    x1=self.conv_t(input)\n",
        "    x1=torch.cat([x1,brain_features],dim=1) \n",
        "\n",
        "    x2=self.conv1(x1)\n",
        "    x2=self.in1(x2)\n",
        "    x2=self.relu1(x2)\n",
        "    x2=torch.cat([x2,params[:,0:x2.shape[1],0:x2.shape[2],0:x2.shape[3]]],dim=1)\n",
        "\n",
        "    x3=self.conv2(x2)\n",
        "    x3=self.in2(x3)\n",
        "    x3=self.relu2(x3)\n",
        "\n",
        "    x4=self.conv3(x3)\n",
        "    x4=self.in3(x4)\n",
        "    x4=self.relu3(x4)\n",
        "\n",
        "    return x4\n",
        "  \n",
        "  def init_weights(self):\n",
        "    for name,module in self.named_modules():\n",
        "      if isinstance(module,nn.Conv2d) or isinstance(module,nn.ConvTranspose2d):\n",
        "        nn.init.xavier_uniform_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "          module.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEEXzhqPPPOu"
      },
      "source": [
        "class ParamNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.block1 = BasicBlock(in_channels=512,out_channels=512)          ## 2*2*512\n",
        "    self.block2 = BasicBlock(in_channels=512,out_channels=512)          ## 4*4*512\n",
        "    self.block3 = BasicBlock(in_channels=512,out_channels=512)          ## 8*8*512\n",
        "    self.block4 = BasicBlock(in_channels=512,out_channels=256)          ## 16*16*256\n",
        "    self.block5 = BasicBlock(in_channels=256,out_channels=128)          ## 32*32*128\n",
        "    self.block6 = BasicBlock(in_channels=128,out_channels=64)           ## 64*64*64\n",
        "    self.block7 = BasicBlock(in_channels=64,out_channels=64)            ## 128*128*64\n",
        "    self.block8 = BasicBlock(in_channels=64,out_channels=32)            ## 256*256*32\n",
        "    self.conv1 = nn.Conv2d(32,16,3,stride=1,padding=1,bias=False)       ## 256*256*16\n",
        "    self.in1=nn.InstanceNorm2d(16)\n",
        "    self.relu1=nn.LeakyReLU(0.2)\n",
        "    self.conv2 = nn.Conv2d(16,1,3,stride=1,padding=1,bias=False)        ## 256*256*1\n",
        "  \n",
        "  def forward(self,brain_features,params):\n",
        "    \n",
        "    input=params[:,0:512,0:1,0:1]\n",
        "    \n",
        "    out1=self.block1(input,brain_features.pop(-1),params)\n",
        "    \n",
        "    out2=self.block2(out1,brain_features.pop(-1),params)\n",
        "\n",
        "    out3=self.block3(out2,brain_features.pop(-1),params)\n",
        "\n",
        "    out4=self.block4(out3,brain_features.pop(-1),params)\n",
        "\n",
        "    out5=self.block5(out4,brain_features.pop(-1),params)\n",
        "\n",
        "    out6=self.block6(out5,brain_features.pop(-1),params)\n",
        "\n",
        "    out7=self.block7(out6,brain_features.pop(-1),params)\n",
        "\n",
        "    out8=self.block8(out7,brain_features.pop(-1),params)\n",
        "\n",
        "    out9=self.relu1(self.in1(self.conv1(out8)))\n",
        "\n",
        "    out10=self.conv2(out9)\n",
        "\n",
        "    ou10=torch.tanh(out10)\n",
        "\n",
        "    return out10\n",
        "    \n",
        "  def init_weights(self):\n",
        "    for name,module in self.named_modules():\n",
        "      if isinstance(module,nn.Conv2d) or isinstance(module,nn.ConvTranspose2d):\n",
        "        nn.init.xavier_uniform_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "          module.bias.data.zero_()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLHFBQXBWZsK"
      },
      "source": [
        "#DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4waVn9lCM8O"
      },
      "source": [
        "############ Unzip data from drive ################################\n",
        "\n",
        "shutil.unpack_archive(config.dir+\"dataset/\"+config.train_file_name+\".zip\",\"\")\n",
        "shutil.unpack_archive(config.dir+\"dataset/\"+config.test_file_name+\".zip\",\"\")\n",
        "shutil.unpack_archive(config.dir+\"dataset/Default.zip\",\"\")\n",
        "train_list=glob.glob(config.train_file_name+'/*')\n",
        "test_list=glob.glob(config.test_file_name+'/*')\n",
        "default_list=glob.glob('Default/*')\n",
        "print(len(train_list),len(test_list),len(default_list))\n",
        "\n",
        "###################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXfqZ9gvCSIs"
      },
      "source": [
        "class MRIDataLoader(data.Dataset):\n",
        "\n",
        "    def __init__(self,config,mode='train'):\n",
        "      \n",
        "      self.mode=mode\n",
        "      self.cfg=config\n",
        "      self.te_def=str(config.te_def)\n",
        "      self.tr_def=str(config.tr_def)\n",
        "      if self.mode=='train':\n",
        "        self.data_path=config.train_file_name+'/'\n",
        "      \n",
        "      elif self.mode=='test':\n",
        "        self.data_path=config.test_file_name+'/'\n",
        "        self.train_data_path=config.train_file_name+'/'\n",
        "      \n",
        "      if self.mode=='train' or self.mode=='test':\n",
        "        self.img_path_list=glob.glob(self.data_path+'T*')\n",
        "\n",
        "        self.input_images=[]\n",
        "        # the dataset contains 24 differnt slices in various parameter settings, we create a list of all 24 default parametered slices in self.input_images below\n",
        "        for i in range(24):\n",
        "          image=cv2.imread(config.train_file_name+'/TR='+self.tr_def+\",TE=\"+self.te_def+',SliceNo='+str(i+1)+',SlicePlane=Axial'+'.jpg',cv2.IMREAD_GRAYSCALE)\n",
        "          image=cv2.resize(image, (256,256), interpolation=cv2.INTER_LINEAR)\n",
        "          image=image.astype(np.float64)\n",
        "          image/=255.0\n",
        "          image=torch.from_numpy(image).unsqueeze(0)\n",
        "          mean=torch.Tensor([0.5])\n",
        "          image=image-mean.expand_as(image)\n",
        "          image=image*2\n",
        "          self.input_images.append(image)\n",
        "      \n",
        "    def __getitem__(self,index):\n",
        "      if self.mode=='train' or self.mode=='test':\n",
        "    \n",
        "        final_image=cv2.imread(self.img_path_list[index],cv2.IMREAD_GRAYSCALE)\n",
        "        final_image=cv2.resize(final_image, (256,256), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        name=self.img_path_list[index].split('/')[1][:-4]\n",
        "        values=name.split(',')\n",
        "        param_tr = float(values[0].split('=')[1])\n",
        "        param_te = float(values[1].split('=')[1])\n",
        "        slice_no = int(values[2].split('=')[1])\n",
        "        slice_plane = values[3].split('=')[1]\n",
        "\n",
        "        final_image=final_image.astype(np.float64)\n",
        "        final_image/=255.0\n",
        "        final_image=torch.from_numpy(final_image).unsqueeze(0)\n",
        "        mean=torch.Tensor([0.5])\n",
        "        final_image=final_image-mean.expand_as(final_image)\n",
        "        final_image=final_image*2\n",
        "\n",
        "        return self.input_images[slice_no-1],final_image,torch.tensor([param_te,param_tr]) # default input image of that slice, ground truth image, parameters of required output image\n",
        "\n",
        "      elif self.mode=='detect':\n",
        "        input_image_path= 'brainweb images/TR=4.5,TE=0.05.jpg'\n",
        "        final_image_path= 'brainweb images/TR=8.0,TE=0.12.jpg'  # if output image is not known, give path of a random image and ignore PSNR and MAE in that case\n",
        "        input_param_tr=float(4.5)\n",
        "        input_param_te=float(0.05)\n",
        "        final_param_tr=float(8.0)\n",
        "        final_param_te=float(0.12)\n",
        "\n",
        "        input_image=cv2.imread(input_image_path,cv2.IMREAD_GRAYSCALE)\n",
        "        final_image=cv2.imread(final_image_path,cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        input_image=cv2.resize(input_image, (256,256), interpolation=cv2.INTER_LINEAR)\n",
        "        final_image=cv2.resize(final_image, (256,256), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        input_image=input_image.astype(np.float64)\n",
        "        input_image/=255.0\n",
        "        input_image=torch.from_numpy(input_image).unsqueeze(0)\n",
        "        mean=torch.Tensor([0.5])\n",
        "        input_image=input_image-mean.expand_as(input_image)\n",
        "        input_image=input_image*2\n",
        "\n",
        "        final_image=final_image.astype(np.float64)\n",
        "        final_image/=255.0\n",
        "        final_image=torch.from_numpy(final_image).unsqueeze(0)\n",
        "        mean=torch.Tensor([0.5])\n",
        "        final_image=final_image-mean.expand_as(final_image)\n",
        "        final_image=final_image*2\n",
        "\n",
        "        return input_image,final_image,torch.tensor([final_param_te,final_param_tr]) # default paramtered input image, ground truth image, parameters of required output image\n",
        "\n",
        "      else:\n",
        "        assert False, \"Unrecognised Mode Detected.\"\n",
        "\n",
        "    def __len__(self):\n",
        "      if self.mode==\"detect\":\n",
        "        return 1\n",
        "      else:\n",
        "        return len(self.img_path_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjgvwZiRCT_g"
      },
      "source": [
        "def train_collate(batch):\n",
        "\n",
        "  input_image_list,final_image_list,params_list=[],[],[]\n",
        "  for i,sample in enumerate(batch):\n",
        "    input_image_list.append(sample[0])\n",
        "    final_image_list.append(sample[1])\n",
        "    params_list.append(sample[2])\n",
        "\n",
        "\n",
        "  input_images=torch.stack(input_image_list)\n",
        "  final_images=torch.stack(final_image_list)\n",
        "  params=torch.stack(params_list)\n",
        "\n",
        "  return input_images,final_images,params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpV69snaCYff"
      },
      "source": [
        "def show_image(img): # to display the output image\n",
        "  img=img.cpu().numpy()\n",
        "  img=img/2 + 0.5\n",
        "  img=img.transpose(1,2,0).squeeze(-1)\n",
        "  img*=255.0\n",
        "  print(img.shape)\n",
        "  cv2_imshow(img)\n",
        "\n",
        "def show_diff_image(req_image,gen_image): # shows difference image between generated image and ground truth output\n",
        "  req_image=req_image.cpu().numpy()\n",
        "  req_image=req_image/2 + 0.5\n",
        "  req_image=req_image.transpose(1,2,0).squeeze(-1)\n",
        "  req_image*=255.0\n",
        "\n",
        "  gen_image=gen_image.cpu().numpy()\n",
        "  gen_image=gen_image/2 + 0.5\n",
        "  gen_image=gen_image.transpose(1,2,0).squeeze(-1)\n",
        "  gen_image*=255.0\n",
        "\n",
        "  diff_image=abs(gen_image-req_image)\n",
        "  min_value,max_value,mean_value=np.min(diff_image),np.max(diff_image),np.mean(diff_image)\n",
        "  print(min_value,max_value,mean_value,diff_image.shape)\n",
        "  diff_image=(diff_image-min_value)/(max_value-min_value)\n",
        "  diff_image*=255.0\n",
        "  cv2_imshow(diff_image)\n",
        "  return min_value,max_value,mean_value\n",
        "\n",
        "def save_weights(state,step_no): #deletes previous weight file and saves latest file for param-net model\n",
        "  weight=glob.glob(config.dir+'weights/latest_KParameter*')\n",
        "  assert len(weight)<=1, \"Multiple weights file, delete others.\"\n",
        "  if weight:\n",
        "    open(weight[0], 'w').close()\n",
        "    os.remove(weight[0])\n",
        "  print(\"Saving weights as latest_KParameter_\"+str(step_no))\n",
        "  torch.save(state,config.dir+\"weights/latest_KParameter_\"+str(step_no)+\".pth.tar\")\n",
        "\n",
        "def PSNR_and_SSIM(imageA, imageB): # calculate psnr and ssim between generated image and ground truth output image\n",
        "\n",
        "    # todo: check if code for ssim calculation is correct\n",
        "\n",
        "    imageA=imageA.cpu().numpy()\n",
        "    imageB=imageB.cpu().numpy()\n",
        "    imageA=imageA.transpose(0,2,3,1)\n",
        "    imageB=imageB.transpose(0,2,3,1)\n",
        "    imageA=imageA/2 + 0.5\n",
        "    imageA*=255.0\n",
        "    imageB=imageB/2 + 0.5\n",
        "    imageB*=255.0\n",
        "    imageA=np.clip(imageA,a_min=0.0,a_max=255.0)\n",
        "    imageB=np.clip(imageB,a_min=0.0,a_max=255.0)\n",
        "\n",
        "    mse = np.mean((imageA - imageB) ** 2)\n",
        "    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n",
        "                  # Therefore PSNR have no importance.\n",
        "        return 100\n",
        "    max_pixel = 255.0\n",
        "    psnr = 20 * log10(max_pixel / sqrt(mse))\n",
        "    ssim_final=0\n",
        "    for i in range(imageA.shape[0]):\n",
        "      ssim_temp = ssim(imageA[i],imageB[i],multichannel=True)\n",
        "      ssim_final+=ssim_temp\n",
        "    \n",
        "    #ssim_final/=imageA.shape[0]\n",
        "    return psnr,ssim_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYoE56D0Cvoh"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw-ybiwWCbBQ"
      },
      "source": [
        "# initialize autoencoder and param-net\n",
        "end_model=END().to(device)\n",
        "param_model=ParamNet().to(device)\n",
        "\n",
        "param_model.train()\n",
        "end_model.eval()\n",
        "\n",
        "config.mode=\"train\"\n",
        "\n",
        "# initialize dataloader and optimizer\n",
        "dataset=MRIDataLoader(config,config.mode)\n",
        "optimizer_param=optim.Adam(param_model.parameters(),lr=config.lr,betas=(0.5, 0.999))\n",
        "train_loader=DataLoader(dataset,config.batch_size,shuffle=True,collate_fn=train_collate)\n",
        "\n",
        "# check if pre-trained weights exists, initialize accordingly\n",
        "config.param_weight_path=glob.glob(config.dir+'weights/latest_KParameter*')\n",
        "assert len(config.param_weight_path)<=1, \"Multiple weight files detected.\"\n",
        "if len(config.param_weight_path)==1:\n",
        "  config.resume=True\n",
        "  config.param_weight_path=config.param_weight_path[0]\n",
        "else:\n",
        "  config.param_weight_path=None\n",
        "  config.resume=False\n",
        "\n",
        "config.end_weight_path=glob.glob(config.dir+'weights/latest_END*')\n",
        "assert len(config.end_weight_path)<=1, \"Multiple END weight files detected.\"\n",
        "if len(config.end_weight_path)==1:\n",
        "  config.end_weight_path=config.end_weight_path[0]\n",
        "  checkpoint_end=torch.load(config.end_weight_path)\n",
        "  end_model.load_state_dict(checkpoint_end['end_model'])\n",
        "  print(\"END weight file found\",config.end_weight_path)\n",
        "else:\n",
        "  assert False,\"No END weight file found\"\n",
        "\n",
        "# if pre-trained weights for param-net were found previously, load the previous state otherwise initialize weights\n",
        "if config.resume:\n",
        "  checkpoint=torch.load(config.param_weight_path)\n",
        "  param_model.load_state_dict(checkpoint['param_model'])\n",
        "  optimizer_param.load_state_dict(checkpoint['optimizer_param'])\n",
        "  print(\"Resuming training with\",config.param_weight_path)\n",
        "else:\n",
        "  param_model.init_weights()\n",
        "  print(\"No Param weights Found, Weights initilaized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1KINUk8Cdmt"
      },
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "training=True\n",
        "step=1\n",
        "if config.resume:\n",
        "  step=checkpoint['step']+1\n",
        "L1=nn.L1Loss()\n",
        "MSE=nn.MSELoss()\n",
        "BCE=nn.BCELoss()\n",
        "time_last=time.time()\n",
        "\n",
        "while training:\n",
        "  for i,(input_images,final_images,params) in enumerate(train_loader):\n",
        "\n",
        "    input_images=Variable(input_images.cuda().type(dtype))\n",
        "    final_images=Variable(final_images.cuda().type(dtype))\n",
        "    params=Variable(params.cuda().type(dtype))\n",
        "\n",
        "    # convert parameters to image shape, with te(echo time) in even channel numbers and tr(repitition time) in odd channel numbers\n",
        "    param_te=(torch.zeros(input_images.size(0),1,256,256).cuda()+params[:,0].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)).detach()\n",
        "    param_tr=(torch.zeros(input_images.size(0),1,256,256).cuda()+params[:,1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)).detach()\n",
        "    \n",
        "    param_input=torch.zeros(input_images.size(0),512,256,256).cuda().type(dtype)\n",
        "    for j in range(512):\n",
        "      if j%2==0:\n",
        "        param_input[:,j,:,:]=param_te.squeeze(1)\n",
        "      else:\n",
        "        param_input[:,j,:,:]=param_tr.squeeze(1)\n",
        "\n",
        "    param_model.zero_grad()\n",
        "\n",
        "    # extract image features of input image from auto-encoder and feed them to param-net along with the parameters\n",
        "    brain_features=end_model(input_images)\n",
        "    \n",
        "    for j,features in enumerate(brain_features):\n",
        "      brain_features[j]=brain_features[j].detach()\n",
        "    param_input=param_input.detach()\n",
        "\n",
        "    out_images=param_model(brain_features,param_input)\n",
        "\n",
        "    loss=MSE(out_images.view(out_images.size(0) ,-1),final_images.view(final_images.size(0),-1))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer_param.step()\n",
        "\n",
        "\n",
        "    this_time=time.time()\n",
        "    if i%10==0:\n",
        "      print(\"Batch No -\",i,\"Completed with time\",this_time-time_last,\".Loss =\",loss.item())\n",
        "    time_last=time.time()\n",
        "\n",
        "  # save state of model after every epoch\n",
        "  print(\"Epoch \",step,\" done.\")\n",
        "  state={'step':step,\n",
        "         'param_model':param_model.state_dict(),\n",
        "         'optimizer_param':optimizer_param.state_dict()}\n",
        "  if step%1==0:\n",
        "    save_weights(state,step)\n",
        "\n",
        "  with torch.no_grad():\n",
        "        print(\"Required -\")\n",
        "        show_image(final_images[0])\n",
        "        print(\"Generated -\")\n",
        "        show_image(out_images[0])\n",
        "  step+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkFzecFOCl7R"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhSuMHlhCg9o"
      },
      "source": [
        "# check for latest weights\n",
        "config.param_weight_path=glob.glob(config.dir+'weights/latest_KParameter_17*')\n",
        "assert len(config.param_weight_path)<=1, \"Multiple weight files detected.\"\n",
        "if len(config.param_weight_path)==1:\n",
        "  config.param_weight_path=config.param_weight_path[0]\n",
        "  print(\"Param weight file found\",config.param_weight_path)\n",
        "else:\n",
        "  assert False,\"No Param Weight file found.\"\n",
        "\n",
        "config.end_weight_path=glob.glob(config.dir+'weights/latest_END*')\n",
        "assert len(config.end_weight_path)<=1, \"Multiple END weight files detected.\"\n",
        "if len(config.end_weight_path)==1:\n",
        "  config.end_weight_path=config.end_weight_path[0]\n",
        "  print(\"END weight file found\",config.end_weight_path)\n",
        "else:\n",
        "  assert False,\"No END weight file found\"\n",
        "\n",
        "#initialize models\n",
        "end_model=END().to(device)\n",
        "param_model=ParamNet().to(device)\n",
        "\n",
        "end_model.eval()\n",
        "param_model.eval()\n",
        "\n",
        "#load weights\n",
        "checkpoint_end=torch.load(config.end_weight_path)\n",
        "end_model.load_state_dict(checkpoint_end['end_model'])\n",
        "\n",
        "checkpoint_param=torch.load(config.param_weight_path)\n",
        "param_model.load_state_dict(checkpoint_param['param_model'])\n",
        "final_loss,final_psnr,final_ssim,total_no=0.0,0.0,0.0,0\n",
        "psnr_values=[]\n",
        "min_diffs,max_diffs,mean_diffs,tot1=[],[],[],0\n",
        "\n",
        "MSE=nn.MSELoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  #initialize dataloader\n",
        "  test_data=MRIDataLoader(config,'test')\n",
        "  test_loader=DataLoader(test_data,config.batch_size,shuffle=True,collate_fn=train_collate)\n",
        "\n",
        "  time_last=time.time()\n",
        "  for i,(input_images,final_images,params) in enumerate(test_loader): \n",
        "\n",
        "    input_images=Variable(input_images.cuda().type(dtype))\n",
        "    final_images=Variable(final_images.cuda().type(dtype))\n",
        "    params=Variable(params.cuda().type(dtype))\n",
        "    \n",
        "    # convert parameters to image shape, with te(echo time) in even channel numbers and tr(repitition time) in odd channel numbers\n",
        "    param_te=(torch.zeros(input_images.size(0),1,256,256).cuda()+params[:,0].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)).detach()\n",
        "    param_tr=(torch.zeros(input_images.size(0),1,256,256).cuda()+params[:,1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)).detach()\n",
        "\n",
        "    param_input=torch.zeros(input_images.size(0),512,256,256).cuda().type(dtype)\n",
        "    for j in range(512):\n",
        "      if j%2==0:\n",
        "        param_input[:,j,:,:]=param_te.squeeze(1)\n",
        "      else:\n",
        "        param_input[:,j,:,:]=param_tr.squeeze(1)\n",
        "\n",
        "    # extract image features of input image from auto-encoder and feed them to param-net along with the parameters\n",
        "    brain_features=end_model(input_images)\n",
        "    \n",
        "    for j,features in enumerate(brain_features):\n",
        "      brain_features[j]=brain_features[j].detach()\n",
        "    param_input=param_input.detach()\n",
        "\n",
        "    out_images=param_model(brain_features,param_input)\n",
        "    loss=MSE(out_images.view(out_images.size(0) ,-1),final_images.view(final_images.size(0),-1))\n",
        "\n",
        "    # calculate psnr and ssim\n",
        "    psnr_temp,ssim_temp=PSNR_and_SSIM(final_images,out_images)\n",
        "    psnr_values.append(psnr_temp)\n",
        "    final_psnr+=psnr_temp\n",
        "    final_ssim+=ssim_temp\n",
        "    final_loss+=loss\n",
        "    total_no+=1\n",
        "\n",
        "    this_time=time.time()\n",
        "    print(\"Batch No -\",i,\"Completed with time\",this_time-time_last,\".Loss =\",loss.item())\n",
        "    time_last=time.time()\n",
        "\n",
        "    # display inputimage, ground truth image, generated image and difference image(|ground truth - generated image|)\n",
        "    if i%10==0:\n",
        "      print(\"Default -\")\n",
        "      show_image(input_images[0])\n",
        "      print(\"Required -\")\n",
        "      show_image(final_images[0])\n",
        "      print(\"Generated -\")\n",
        "      show_image(out_images[0])\n",
        "      print(\"Differnce Image -\")\n",
        "      curr_min,curr_max,curr_mean=show_diff_image(final_images[0],out_images[0])\n",
        "      min_diffs.append(curr_min)\n",
        "      max_diffs.append(curr_max)\n",
        "      mean_diffs.append(curr_mean)\n",
        "      tot1+=1\n",
        "      print(\"TE =\",params[0,0],\"TR =\",params[0,1])\n",
        "\n",
        "# display average psnr, avg pixel difference\n",
        "final_loss/=total_no\n",
        "final_psnr/=total_no\n",
        "final_ssim/=total_no\n",
        "print(\"Final Loss =\",final_loss,\"Final PSNR =\",final_psnr,\"Final SSIM =\",final_ssim)\n",
        "print(\"PSNR Array =\",psnr_values)\n",
        "avg_min_diff=np.sum(min_diffs)/tot1\n",
        "avg_max_diff=np.sum(max_diffs)/tot1\n",
        "avg_mean_diff=np.sum(mean_diffs)/tot1\n",
        "print(\"Avg Min Difference =\",avg_min_diff,\"Avg Max Difference =\",avg_max_diff,\"Avg Mean Difference =\",avg_mean_diff)\n",
        "\n",
        "# display mean and std dev of pixel differences\n",
        "mean = sum(mean_diffs) / len(mean_diffs)\n",
        "variance = sum([((x - mean) ** 2) for x in mean_diffs]) / len(mean_diffs)\n",
        "res = variance ** 0.5\n",
        "print(\"Mean =\",mean,\"Res =\",res)\n",
        "\n",
        "# display mean and std dev of psnr\n",
        "mean = sum(psnr_values) / len(psnr_values)\n",
        "variance = sum([((x - mean) ** 2) for x in psnr_values]) / len(psnr_values)\n",
        "res = variance ** 0.5\n",
        "print(\"Mean =\",mean,\"Res =\",res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqcYVfWc9Ckg"
      },
      "source": [
        "#Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkTtogJr9A4W"
      },
      "source": [
        "# check for latest weights\n",
        "config.param_weight_path=glob.glob(config.dir+'weights/latest_KParameter_17*')\n",
        "assert len(config.param_weight_path)<=1, \"Multiple weight files detected.\"\n",
        "if len(config.param_weight_path)==1:\n",
        "  config.param_weight_path=config.param_weight_path[0]\n",
        "  print(\"Param weight file found\",config.param_weight_path)\n",
        "else:\n",
        "  assert False,\"No Param Weight file found.\"\n",
        "\n",
        "config.end_weight_path=glob.glob(config.dir+'weights/latest_END*')\n",
        "assert len(config.end_weight_path)<=1, \"Multiple END weight files detected.\"\n",
        "if len(config.end_weight_path)==1:\n",
        "  config.end_weight_path=config.end_weight_path[0]\n",
        "  print(\"END weight file found\",config.end_weight_path)\n",
        "else:\n",
        "  assert False,\"No END weight file found\"\n",
        "\n",
        "#initialize models\n",
        "end_model=END().to(device)\n",
        "param_model=ParamNet().to(device)\n",
        "\n",
        "end_model.eval()\n",
        "param_model.eval()\n",
        "\n",
        "#load weights\n",
        "checkpoint_end=torch.load(config.end_weight_path)\n",
        "end_model.load_state_dict(checkpoint_end['end_model'])\n",
        "\n",
        "checkpoint_param=torch.load(config.param_weight_path)\n",
        "param_model.load_state_dict(checkpoint_param['param_model'])\n",
        "final_loss,final_psnr,final_ssim,total_no=0.0,0.0,0.0,0\n",
        "psnr_values=[]\n",
        "\n",
        "MSE=nn.MSELoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  #initialize dataloader\n",
        "  test_data=MRIDataLoader(config,'detect')\n",
        "  test_loader=DataLoader(test_data,config.batch_size,shuffle=True,collate_fn=train_collate)\n",
        "  time_last=time.time()\n",
        "  for i,(input_images,final_images,params) in enumerate(test_loader): \n",
        "\n",
        "    input_images=Variable(input_images.cuda().type(dtype))\n",
        "    final_images=Variable(final_images.cuda().type(dtype))\n",
        "    params=Variable(params.cuda().type(dtype))\n",
        "\n",
        "    # convert parameters to image shape, with te(echo time) in even channel numbers and tr(repitition time) in odd channel numbers\n",
        "    param_te=(torch.zeros(input_images.size(0),1,256,256).cuda()+params[:,0].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)).detach()\n",
        "    param_tr=(torch.zeros(input_images.size(0),1,256,256).cuda()+params[:,1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)).detach()\n",
        "\n",
        "    param_input=torch.zeros(input_images.size(0),512,256,256).cuda().type(dtype)\n",
        "    for j in range(512):\n",
        "      if j%2==0:\n",
        "        param_input[:,j,:,:]=param_te.squeeze(1)\n",
        "      else:\n",
        "        param_input[:,j,:,:]=param_tr.squeeze(1)\n",
        "    \n",
        "    # extract image features of input image from auto-encoder and feed them to param-net along with the parameters\n",
        "    brain_features=end_model(input_images)\n",
        "    \n",
        "    for j,features in enumerate(brain_features):\n",
        "      brain_features[j]=brain_features[j].detach()\n",
        "    param_input=param_input.detach()\n",
        "\n",
        "    out_images=param_model(brain_features,param_input)\n",
        "    loss=MSE(out_images.view(out_images.size(0) ,-1),final_images.view(final_images.size(0),-1))\n",
        "\n",
        "    # calculate psnr and ssim\n",
        "    psnr_temp,ssim_temp=PSNR_and_SSIM(final_images,out_images)\n",
        "    psnr_values.append(psnr_temp)\n",
        "    final_psnr+=psnr_temp\n",
        "    final_ssim+=ssim_temp\n",
        "    final_loss+=loss\n",
        "    total_no+=1\n",
        "\n",
        "    this_time=time.time()\n",
        "    print(\"Batch No -\",i,\"Completed with time\",this_time-time_last,\".Loss =\",loss.item())\n",
        "    time_last=time.time()\n",
        "    \n",
        "    # display inputimage, ground truth image and generated image\n",
        "    if i%10==0:\n",
        "      print(\"Default -\")\n",
        "      show_image(input_images[0])\n",
        "      print(\"Required -\")\n",
        "      show_image(final_images[0])\n",
        "      print(\"Generated -\")\n",
        "      show_image(out_images[0])\n",
        "      print(\"TE =\",params[0,0],\"TR =\",params[0,1])\n",
        "\n",
        "final_loss/=total_no\n",
        "final_psnr/=total_no\n",
        "final_ssim/=total_no\n",
        "print(\"Final Loss =\",final_loss,\"Final PSNR =\",final_psnr,\"Final SSIM =\",final_ssim)\n",
        "print(\"PSNR Array =\",psnr_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGgOEwfZA42p"
      },
      "source": [
        "#Train END(Autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L58_rqmtYdhR"
      },
      "source": [
        "class END1(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.conv1=nn.Conv2d(1,64,3,stride=1,padding=1,bias=False)\n",
        "    self.relu1=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv2=nn.Conv2d(64,64,3,stride=2,padding=1,bias=False)\n",
        "    self.bn2=nn.BatchNorm2d(64,momentum=0.5)\n",
        "    self.relu2=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv3=nn.Conv2d(64,128,3,stride=2,padding=1,bias=False)\n",
        "    self.bn3=nn.BatchNorm2d(128,momentum=0.5)\n",
        "    self.relu3=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv4=nn.Conv2d(128,256,3,stride=2,padding=1,bias=False)\n",
        "    self.bn4=nn.BatchNorm2d(256,momentum=0.5)\n",
        "    self.relu4=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv5=nn.Conv2d(256,512,3,stride=2,padding=1,bias=False)\n",
        "    self.bn5=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu5=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv6=nn.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
        "    self.bn6=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu6=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv7=nn.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
        "    self.bn7=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu7=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv8=nn.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
        "    self.bn8=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu8=nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.conv9=nn.ConvTranspose2d(512,512,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn9=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu9=nn.ReLU()\n",
        "      \n",
        "    self.conv10=nn.ConvTranspose2d(512,512,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn10=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu10=nn.ReLU()\n",
        "\n",
        "    self.conv11=nn.ConvTranspose2d(512,512,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn11=nn.BatchNorm2d(512,momentum=0.5)\n",
        "    self.relu11=nn.ReLU()\n",
        "\n",
        "    self.conv12=nn.ConvTranspose2d(512,256,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn12=nn.BatchNorm2d(256,momentum=0.5)\n",
        "    self.relu12=nn.ReLU()\n",
        "\n",
        "    self.conv13=nn.ConvTranspose2d(256,128,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn13=nn.BatchNorm2d(128,momentum=0.5)\n",
        "    self.relu13=nn.ReLU()\n",
        "\n",
        "    self.conv14=nn.ConvTranspose2d(128,64,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn14=nn.BatchNorm2d(64,momentum=0.5)\n",
        "    self.relu14=nn.ReLU()\n",
        "\n",
        "    self.conv15=nn.ConvTranspose2d(64,32,3,stride=2,padding=1,output_padding=1,bias=False)\n",
        "    self.bn15=nn.BatchNorm2d(32,momentum=0.5)\n",
        "    self.relu15=nn.ReLU()\n",
        "      \n",
        "    self.conv16=nn.Conv2d(32,16,3,stride=1,padding=1,bias=False)\n",
        "    self.bn16=nn.BatchNorm2d(16,momentum=0.5)\n",
        "    self.relu16=nn.ReLU()\n",
        "\n",
        "    self.conv17=nn.Conv2d(16,1,1,stride=1,bias=False)\n",
        "\n",
        "  def forward(self,img):\n",
        "\n",
        "    x1=self.relu1(self.conv1(img))\n",
        "\n",
        "    x2=self.relu2(self.bn2(self.conv2(x1)))\n",
        "\n",
        "    x3=self.relu3(self.bn3(self.conv3(x2)))\n",
        "\n",
        "    x4=self.relu4(self.bn4(self.conv4(x3)))\n",
        "\n",
        "    x5=self.relu5(self.bn5(self.conv5(x4)))\n",
        "\n",
        "    x6=self.relu6(self.bn6(self.conv6(x5)))\n",
        "\n",
        "    x7=self.relu7(self.bn7(self.conv7(x6)))\n",
        "\n",
        "    x8=self.relu8(self.bn8(self.conv8(x7)))\n",
        "\n",
        "    x9=self.relu9(self.bn9(self.conv9(x8)))\n",
        "\n",
        "    x10=self.relu10(self.bn10(self.conv10(x9)))\n",
        "\n",
        "    x11=self.relu11(self.bn11(self.conv11(x10)))\n",
        "\n",
        "    x12=self.relu12(self.bn12(self.conv12(x11)))\n",
        "\n",
        "    x13=self.relu13(self.bn13(self.conv13(x12)))\n",
        "\n",
        "    x14=self.relu14(self.bn14(self.conv14(x13)))\n",
        "\n",
        "    x15=self.relu15(self.bn15(self.conv15(x14)))\n",
        "\n",
        "    x16=self.relu16(self.bn16(self.conv16(x15)))\n",
        "        \n",
        "    x17=self.conv17(x16)\n",
        "    x17=torch.tanh(x17)\n",
        "      \n",
        "    return x17\n",
        "  \n",
        "  def init_weights(self):\n",
        "    for name,module in self.named_modules():\n",
        "      if isinstance(module,nn.Conv2d) or isinstance(module,nn.ConvTranspose2d):\n",
        "        nn.init.xavier_uniform_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "          module.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNPbJnWcA7Nc"
      },
      "source": [
        "############ Unzip data from drive ################################\n",
        "\n",
        "shutil.unpack_archive(config.dir+\"dataset/\"+config.train_file_name+\".zip\",\"\")\n",
        "shutil.unpack_archive(config.dir+\"dataset/\"+config.test_file_name+\".zip\",\"\")\n",
        "shutil.unpack_archive(config.dir+\"dataset/Default.zip\",\"\")\n",
        "shutil.unpack_archive(config.dir+'dataset/END data.zip','')\n",
        "train_list=glob.glob(config.train_file_name+'/*')\n",
        "test_list=glob.glob(config.test_file_name+'/*')\n",
        "default_list=glob.glob('Default/*')\n",
        "end_data_list=glob.glob('END data/*')\n",
        "print(len(train_list),len(test_list),len(default_list),len(end_data_list))\n",
        "\n",
        "###################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTj_Jpn1BASi"
      },
      "source": [
        "class END_MRILoader(data.Dataset):\n",
        "  def __init__(self,mode=\"train\"):\n",
        "    \n",
        "    if mode==\"train\":\n",
        "      self.img_path_list=glob.glob(config.train_file_name+'/*')\n",
        "    elif mode=='test':\n",
        "      self.img_path_list=glob.glob(config.test_file_name+'/*')\n",
        "    elif mode=='default':\n",
        "      self.img_path_list=glob.glob('Default/*')\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    \n",
        "    img=cv2.imread(self.img_path_list[index],cv2.IMREAD_GRAYSCALE)\n",
        "    img=cv2.resize(img, (256,256), interpolation=cv2.INTER_LINEAR)\n",
        "    img=img.astype(np.float64)\n",
        "    img/=255.0\n",
        "    img=torch.from_numpy(img).unsqueeze(0)\n",
        "    mean=torch.Tensor([0.5])\n",
        "    img=img-mean.expand_as(img)\n",
        "    img*=2\n",
        "\n",
        "    return img\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.img_path_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_qJUrgBCF3"
      },
      "source": [
        "def train_collate_end(batch):\n",
        "\n",
        "  images=torch.stack(batch)\n",
        "\n",
        "  return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q41turryBDjT"
      },
      "source": [
        "def show_image_end(img):\n",
        "  img=img.cpu().numpy()\n",
        "  img=img/2 + 0.5\n",
        "  img=img.transpose(1,2,0).squeeze(-1)\n",
        "  img*=255.0\n",
        "  cv2_imshow(img)\n",
        "\n",
        "def save_weights_end(state,step_no):\n",
        "  weight=glob.glob(config.dir+'weights/latest_END_*')\n",
        "  assert len(weight)<=1, \"Multiple weights file, delete others.\"\n",
        "  if weight:\n",
        "    open(weight[0], 'w').close()\n",
        "    os.remove(weight[0])\n",
        "  print(\"Saving weights as latest_END_\"+str(step_no))\n",
        "  torch.save(state,config.dir+\"weights/latest_END_\"+str(step_no)+\".pth.tar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uM7NL1jBE4q"
      },
      "source": [
        "# initialize model\n",
        "end_model=END1().to(device)\n",
        "\n",
        "end_model.train()\n",
        "\n",
        "# load dataloader and optimizer\n",
        "end_dataset=END_MRILoader(\"train\")\n",
        "optimizer_end=optim.Adam(end_model.parameters(),lr=config.lr,betas=(0.5, 0.999))\n",
        "end_train_loader=DataLoader(end_dataset,config.batch_size,shuffle=True,collate_fn=train_collate_end)\n",
        "\n",
        "# check if pretrained weights exist, if they do, load them else initialize weights\n",
        "config.end_weight_path=glob.glob(config.dir+'weights/latest_END*')\n",
        "assert len(config.end_weight_path)<=1, \"Multiple END weight files detected.\"\n",
        "if len(config.end_weight_path)==1:\n",
        "  config.end_weight_path=config.end_weight_path[0]\n",
        "  checkpoint_end=torch.load(config.end_weight_path)\n",
        "  end_model.load_state_dict(checkpoint_end['end_model'])\n",
        "  optimizer_end.load_state_dict(checkpoint_end['optimizer_end'])\n",
        "  print(\"Resuming training with\",config.end_weight_path)\n",
        "else:\n",
        "  config.end_weight_path=None\n",
        "  end_model.init_weights()\n",
        "  print(\"No END weights Found, Weights initilaized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R35LB1VBGaD"
      },
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "training=True\n",
        "step=1\n",
        "if config.end_weight_path is not None:\n",
        "  step=checkpoint_end['step']+1\n",
        "  print(\"Start Step for Resume training =\",step)\n",
        "\n",
        "MSE=nn.MSELoss()\n",
        "time_last=time.time()\n",
        "\n",
        "while training:\n",
        "  for i,(images) in enumerate(end_train_loader):\n",
        "\n",
        "    images=Variable(images.cuda().type(dtype))\n",
        "\n",
        "    end_model.zero_grad()\n",
        "\n",
        "    out_images=end_model(images)\n",
        "    \n",
        "    loss=MSE(out_images.view(out_images.size(0) ,-1),images.view(images.size(0),-1))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer_end.step()\n",
        "    \n",
        "    this_time=time.time()\n",
        "    if i%20==0:\n",
        "      print(\"Batch No -\",i,\"Completed with time\",this_time-time_last,\".Loss =\",loss.item())\n",
        "    time_last=time.time()\n",
        "\n",
        "  # save weights after every epoch\n",
        "  print(\"Epoch \",step,\" done.\")\n",
        "  state={'step':step,\n",
        "         'end_model':end_model.state_dict(),\n",
        "         'optimizer_end':optimizer_end.state_dict()}\n",
        "  save_weights_end(state,step)\n",
        "  with torch.no_grad():\n",
        "        print(\"Required -\")\n",
        "        show_image_end(images[0])\n",
        "        print(\"Generated -\")\n",
        "        show_image_end(out_images[0])\n",
        "  step+=1\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhJO3I8DBIPg"
      },
      "source": [
        "###################### Test END(Autoencoder) model ##############################\n",
        "\n",
        "# check if weights exist, if they don't, raise a error\n",
        "config.weight_path=glob.glob('drive/MyDrive/Projects/K-Parameter/weights/latest_END_*')\n",
        "assert len(config.weight_path)<=1, \"Multiple weight files detected.\"\n",
        "if len(config.weight_path)==1:\n",
        "  config.weight_path=config.weight_path[0]\n",
        "  print(\"Weight file found -\",config.weight_path)\n",
        "else:\n",
        "  assert False, \"No weight file detected\"\n",
        "\n",
        "#initialize model\n",
        "end_model=END1().to(device)\n",
        "\n",
        "end_model.eval()\n",
        "\n",
        "#load weights\n",
        "checkpoint=torch.load(config.weight_path)\n",
        "end_model.load_state_dict(checkpoint['end_model'])\n",
        "\n",
        "MSE=nn.MSELoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  #load dataloader\n",
        "  end_test_data=END_MRILoader('default')\n",
        "  end_test_loader=DataLoader(end_test_data,config.batch_size,shuffle=True,collate_fn=train_collate_end)\n",
        "  time_last=time.time()\n",
        "  for i,(images) in enumerate(end_test_loader): \n",
        "\n",
        "    images=Variable(images.cuda().type(dtype))\n",
        "    \n",
        "    out_images=end_model(images)\n",
        "\n",
        "    loss=MSE(out_images.view(out_images.size(0) ,-1),images.view(images.size(0),-1))\n",
        "\n",
        "    this_time=time.time()\n",
        "    print(\"Batch No -\",i,\"Completed with time\",this_time-time_last,\".Loss =\",loss.item())\n",
        "    time_last=time.time()\n",
        "    \n",
        "    for j in range(images.size(0)):\n",
        "      print(\"Required -\")\n",
        "      show_image(images[j])\n",
        "      print(\"Generated -\")\n",
        "      show_image(out_images[j])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}